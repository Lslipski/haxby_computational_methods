{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "from mvpa2.base import externals\n",
    "from mvpa2.base.param import Parameter\n",
    "from mvpa2.base.constraints import *\n",
    "from mvpa2.base.dataset import hstack\n",
    "from mvpa2.base.types import is_datasetlike\n",
    "\n",
    "from mvpa2.datasets import Dataset\n",
    "\n",
    "from mvpa2.mappers.zscore import zscore\n",
    "from mvpa2.mappers.fxy import FxyMapper\n",
    "from mvpa2.mappers.svd import SVDMapper\n",
    "\n",
    "from mvpa2.measures.searchlight import Searchlight\n",
    "from mvpa2.measures.base import Measure\n",
    "\n",
    "from mvpa2.algorithms.searchlight_hyperalignment import SearchlightHyperalignment\n",
    "# experimental sparse matrix usage for faster computing with possible extra mem load\n",
    "# from searchlight_hyperalignment import SearchlightHyperalignment\n",
    "\n",
    "from mvpa2.algorithms.hyperalignment import Hyperalignment\n",
    "\n",
    "from mvpa2.support.due import due, Doi\n",
    "\n",
    "if externals.exists('h5py'):\n",
    "    from mvpa2.base.hdf5 import h5save, h5load\n",
    "\n",
    "if __debug__:\n",
    "    from mvpa2.base import debug\n",
    "    if 'CHPAL' in debug.active:\n",
    "        def _chpaldebug(msg):\n",
    "            debug('CHPAL', \"%s\" % msg)\n",
    "    else:\n",
    "        def _chpaldebug(*args):\n",
    "            return None\n",
    "else:\n",
    "    def _chpaldebug(*args):\n",
    "        return None\n",
    "\n",
    "class MeanFeatureMeasure(Measure):\n",
    "    \"\"\"Mean group feature measure\n",
    "    Because the vanilla one doesn't want to work for me (Swaroop).\n",
    "    TODO: figure out why \"didn't work\" exactly, and adjust description\n",
    "    and possibly name above\n",
    "    \"\"\"\n",
    "\n",
    "    is_trained = True\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        Measure.__init__(self, **kwargs)\n",
    "\n",
    "    def _call(self, dataset):\n",
    "        return Dataset(samples=np.mean(dataset.samples, axis=1))\n",
    "    \n",
    "def _get_connectomes(self, datasets):\n",
    "    params = self.params\n",
    "    # If no precomputed connectomes are supplied, compute them.\n",
    "    if params.connectomes is not None and os.path.exists(params.connectomes):\n",
    "        _chpaldebug(\"Loading pre-computed connectomes from %s\" % params.connectomes)\n",
    "        connectomes = h5load(params.connectomes)\n",
    "        return connectomes\n",
    "    connectivity_mapper = FxyMapper(params.conn_metric)\n",
    "    # Initializing datasets with original anatomically aligned datasets\n",
    "    mfm = MeanFeatureMeasure()\n",
    "    # TODO Handle seed_radius if seed queryengines are not provided\n",
    "    seed_radius = params.seed_radius\n",
    "    _chpaldebug(\"Performing surface connectivity hyperalignment with seeds\")\n",
    "    _chpaldebug(\"Computing connectomes.\")\n",
    "    ndatasets = len(datasets)\n",
    "    if params.seed_queryengines is None:\n",
    "        raise NotImplementedError(\"For now, we need seed queryengines.\")\n",
    "    qe_all = super(ConnectivityHyperalignment, self)._get_trained_queryengines(\n",
    "        datasets, params.seed_queryengines, seed_radius, params.ref_ds)\n",
    "    # If seed_indices are not supplied, use all as centers\n",
    "    if not params.seed_indices:\n",
    "        roi_ids = super(ConnectivityHyperalignment, self)._get_verified_ids(qe_all)\n",
    "    else:\n",
    "        roi_ids = params.seed_indices\n",
    "    if len(qe_all) == 1:\n",
    "        qe_all *= ndatasets\n",
    "    # Computing Seed means to be used for aligning seed features\n",
    "    seed_means = [self._get_seed_means(MeanFeatureMeasure(), qe, ds, params.seed_indices)\n",
    "                  for qe, ds in zip(qe_all, datasets)]\n",
    "    if params.npcs is None:\n",
    "        conn_targets = []\n",
    "        for seed_mean in seed_means:\n",
    "            zscore(seed_mean, chunks_attr=None)\n",
    "            conn_targets.append(seed_mean)\n",
    "    else:\n",
    "        # compute all PC-seed connectivity in each subject\n",
    "        # 1. make common model SVs in each seed SL based on connectivity to seed_means\n",
    "        # 2. Use these SVs for computing connectomes\n",
    "        _chpaldebug(\"Aligning SVs in each searchlight across subjects\")\n",
    "        # Looping over all seeds in which SVD is done\n",
    "        pc_data = [[] for isub in range(ndatasets)]\n",
    "        sl_common_models = dict()\n",
    "        if params.common_model is not None and os.path.exists(params.common_model):\n",
    "            _chpaldebug(\"Loading common model from %s\" % params.common_model)\n",
    "            common_model = h5load(params.common_model)\n",
    "            sl_common_models = common_model['local_models']\n",
    "        for inode in roi_ids:\n",
    "            # For each SL, computing connectivity of features to seed means\n",
    "            # This line below doesn't need common model\n",
    "            sl_connectomes = self._get_sl_connectomes(seed_means, qe_all, datasets,\n",
    "                                                      inode, connectivity_mapper)\n",
    "            # Hyperalign connectomes in SL\n",
    "            # XXX TODO Common model input to below function should be updated.\n",
    "            local_common_model = sl_common_models[inode][:, :params.npcs] \\\n",
    "                                    if params.common_model else None\n",
    "            sl_hmappers, svm, sl_common_model = self._get_hypesvs(sl_connectomes,\n",
    "                                            local_common_model=local_common_model)\n",
    "            if sl_common_model is not None:\n",
    "                sl_common_models[inode] = sl_common_model\n",
    "            # make common model SV timeseries data in each subject\n",
    "            for sd, slhm, qe, pcd in zip(datasets, sl_hmappers, qe_all, pc_data):\n",
    "                sd_svs = slhm.forward(sd[:, qe[inode]])\n",
    "                zscore(sd_svs, chunks_attr=None)\n",
    "                if svm is not None:\n",
    "                    sd_svs = svm.forward(sd_svs)\n",
    "                    sd_svs = sd_svs[:, :params.npcs]\n",
    "                    zscore(sd_svs, chunks_attr=None)\n",
    "                pcd.append(sd_svs)\n",
    "        if params.save_model is not None:\n",
    "            # TODO: should use debug\n",
    "            print('Saving local models to %s' % params.save_model)\n",
    "            h5save(params.save_model, sl_common_models)\n",
    "        pc_data = [hstack(pcd) for pcd in pc_data]\n",
    "        conn_targets = pc_data\n",
    "        #print pc_data[-1]\n",
    "    # compute connectomes using connectivity targets (PCs or seed means)\n",
    "    connectomes = []\n",
    "    if params.common_model is not None and os.path.exists(params.common_model):\n",
    "        # TODO: should use debug\n",
    "        print('Loading from saved common model: %s' % params.common_model)\n",
    "        connectome_model = common_model['connectome_model']\n",
    "        connectomes.append(connectome_model)\n",
    "    for t_, ds in zip(conn_targets, datasets):\n",
    "        connectivity_mapper.train(t_)\n",
    "        connectome = connectivity_mapper.forward(ds)\n",
    "        t_ = None\n",
    "        connectome.fa = ds.fa\n",
    "        if connectome.samples.dtype == 'float64':\n",
    "            connectome.samples = connectome.samples.astype('float32')\n",
    "        zscore(connectome, chunks_attr=None)\n",
    "        connectomes.append(connectome)\n",
    "    if params.connectomes is not None and not os.path.exists(params.connectomes):\n",
    "        _chpaldebug(\"Saving connectomes to \", params.connectomes)\n",
    "        h5save(params.connectomes, connectomes)\n",
    "    return connectomes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/dartfs-hpc/scratch/psyc164/tcat/data/budapest/'\n",
    "save_path = '/dartfs-hpc/scratch/psyc164/tcat/data'\n",
    "hemis = ['L'] #'R'\n",
    "subids = [5, 7, 9] #, 10, 13, 20, 21, 24, 29, 34, 52, 114, 120, 134, 142, 278, 416, 499, 522, 535, 560]\n",
    "files = []\n",
    "\n",
    "for hemi in hemis:\n",
    "    for subid in subids:\n",
    "        sub = '{:0>6}'.format(subid)\n",
    "        fn = os.path.join(data_path + 'sub-rid' + sub + '*' + hemi + '.npy')\n",
    "        files.append(sorted(glob.glob(fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = range(1,21)\n",
    "ds = None\n",
    "for x in range(len(files)):\n",
    "    d = mv.Dataset(np.load(files[x][0]))#mv.gifti_dataset(files[x], targets=targets)\n",
    "    if ds is None:\n",
    "        ds = d\n",
    "    else:      \n",
    "        ds = mv.vstack((ds,d))\n",
    "ds.fa['node_indices'] = range(ds.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9156, 9372)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_get_connectomes() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-dbd6b48e4887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_get_connectomes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: _get_connectomes() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "_get_connectomes(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
